{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install --quiet \"pandas\" \"torch\" \"torchvision\" \"ipython[notebook]\" \"seaborn\" \"pytorch-lightning>=1.4\" \"torchmetrics>=0.6\" \"lightning-bolts\" \"torch-lr-finder\" \"grad-cam\" \"gradio\" \"torchinfo\"","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:36:46.149135Z","iopub.execute_input":"2023-09-15T23:36:46.149890Z","iopub.status.idle":"2023-09-15T23:37:29.626989Z","shell.execute_reply.started":"2023-09-15T23:36:46.149855Z","shell.execute_reply":"2023-09-15T23:37:29.625581Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! git clone https://github.com/niharikavadapalli/ERA-V1.git\n!git -C ERA-V1 pull\n\nimport sys\nsys.path.append(\"ERA-V1/ERA-V1-Session-17\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:38:27.428370Z","iopub.execute_input":"2023-09-15T23:38:27.428888Z","iopub.status.idle":"2023-09-15T23:38:34.014752Z","shell.execute_reply.started":"2023-09-15T23:38:27.428859Z","shell.execute_reply":"2023-09-15T23:38:34.013573Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'ERA-V1'...\nremote: Enumerating objects: 1269, done.\u001b[K\nremote: Counting objects: 100% (461/461), done.\u001b[K\nremote: Compressing objects: 100% (423/423), done.\u001b[K\nremote: Total 1269 (delta 77), reused 413 (delta 37), pack-reused 808\u001b[K\nReceiving objects: 100% (1269/1269), 71.01 MiB | 23.85 MiB/s, done.\nResolving deltas: 100% (476/476), done.\nAlready up to date.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformer_models import GPT\nfrom transformers import AutoTokenizer  # pip install transformers\nfrom GPT.utils import (\n    BATCH_SIZE,\n    BLOCK_SIZE,\n    DEVICE,\n    DROPOUT,\n    LEARNING_RATE,\n    NUM_EMBED,\n    NUM_HEAD,\n    NUM_LAYER,\n    MAX_ITER,\n    EVAL_INTER,\n    encode,\n    decode,\n    get_batch,\n    save_model_to_chekpoint,\n    estimate_loss,\n)\n\n# load model from checkpoint\n# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n\n# example to decode sequence\n# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n# max_new_tokens=20)[0].tolist()\n# print(decode(vocab=vocab, enc_sec=enc_sec))\n\n# raw data\npath_do_data = \"ERA-V1/ERA-V1-Session-17/GPT/data/english.txt\"\ndata_raw = open(path_do_data, encoding=\"utf-8\").read()\n# we use pretrained BERT tokenizer for performance improvements\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nvocab_size = tokenizer.vocab_size\n# data_raw = data_raw[4000000:] # short dataset\n\n# train/val split\ndata = encode(text_seq=data_raw, tokenizer=tokenizer)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# train a new model\nmodel = GPT(\n    vocab_size=vocab_size,\n    num_embed=NUM_EMBED,\n    block_size=BLOCK_SIZE,\n    num_heads=NUM_HEAD,\n    num_layers=NUM_LAYER,\n    dropout=DROPOUT,\n)\n# load model to GPU if available\nm = model.to(DEVICE)\n# print the number of parameters in the model\nprint(\n    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:39:23.739154Z","iopub.execute_input":"2023-09-15T23:39:23.739859Z","iopub.status.idle":"2023-09-15T23:39:28.512341Z","shell.execute_reply.started":"2023-09-15T23:39:23.739824Z","shell.execute_reply":"2023-09-15T23:39:28.510352Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (37443 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Model with 89.48M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"# optimizer takes the model's parameters and the learning rate as input,\n# and updates the parameters during the training process in order to\n# minimize the loss function.\noptimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\nMAX_ITER = 500\nfor step in range(MAX_ITER):\n\n    # every EVAL_INTER evaluate the loss on train and val sets\n    if (step + 1) % 100 == 0 :\n        loss_train = estimate_loss(\n            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n        )\n        loss_val = estimate_loss(\n            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n        )\n        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n\n    # sample a batch of data\n    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n    logits, loss = m.forward(xb, yb)\n    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n    optimizer.zero_grad(set_to_none=True)\n    # backward() method on the loss variable calculates the gradients \n    # of the loss with respect to the model's parameters.\n    loss.backward()\n    # step() method on the optimizer updates the model's parameters \n    # using the calculated gradients, in order to minimize the loss.\n    optimizer.step()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:43:06.787070Z","iopub.execute_input":"2023-09-15T23:43:06.787488Z","iopub.status.idle":"2023-09-15T23:45:29.919805Z","shell.execute_reply.started":"2023-09-15T23:43:06.787454Z","shell.execute_reply":"2023-09-15T23:45:29.918703Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"step         99 | train loss 0.5855 | val loss 8.1387\nstep        199 | train loss 0.3416 | val loss 8.8287\nstep        299 | train loss 0.2534 | val loss 9.0215\nstep        399 | train loss 0.2046 | val loss 9.5519\nstep        499 | train loss 0.1808 | val loss 9.8571\n","output_type":"stream"}]},{"cell_type":"code","source":"# generate some output based on the context\ncontext = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\nprint(\n    decode(\n        enc_sec=m.generate(idx=context, max_new_tokens=100, block_size=BLOCK_SIZE)[0],\n        tokenizer=tokenizer,\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:45:37.284677Z","iopub.execute_input":"2023-09-15T23:45:37.285314Z","iopub.status.idle":"2023-09-15T23:45:48.049552Z","shell.execute_reply.started":"2023-09-15T23:45:37.285246Z","shell.execute_reply":"2023-09-15T23:45:48.048411Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[PAD] model, which learn an isolated repo stored on the data from what between? and output for the current repo. when the distribution of the input data, which a few more than once. and falls in the next, we should be able to figure, and then proceed to be able to make something like this : and then perform operations, but the receptive fields the most important distinction behind softmax function, the two types of relationships in the data. without activation functions, a neural network would\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}