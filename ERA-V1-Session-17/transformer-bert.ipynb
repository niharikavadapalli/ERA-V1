{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-15T21:53:49.311665Z","iopub.execute_input":"2023-09-15T21:53:49.312494Z","iopub.status.idle":"2023-09-15T21:53:49.694257Z","shell.execute_reply.started":"2023-09-15T21:53:49.312459Z","shell.execute_reply":"2023-09-15T21:53:49.693210Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! pip install --quiet \"pandas\" \"torch\" \"torchvision\" \"ipython[notebook]\" \"seaborn\" \"pytorch-lightning>=1.4\" \"torchmetrics>=0.6\" \"lightning-bolts\" \"torch-lr-finder\" \"grad-cam\" \"gradio\" \"torchinfo\"","metadata":{"execution":{"iopub.status.busy":"2023-09-15T21:54:42.612179Z","iopub.execute_input":"2023-09-15T21:54:42.612677Z","iopub.status.idle":"2023-09-15T21:55:22.497413Z","shell.execute_reply.started":"2023-09-15T21:54:42.612645Z","shell.execute_reply":"2023-09-15T21:55:22.496032Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"! git clone https://github.com/niharikavadapalli/ERA-V1.git\n!git -C ERA-V1 pull\n\nimport sys\nsys.path.append(\"ERA-V1/ERA-V1-Session-17\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T21:56:15.690109Z","iopub.execute_input":"2023-09-15T21:56:15.690477Z","iopub.status.idle":"2023-09-15T21:56:21.413602Z","shell.execute_reply.started":"2023-09-15T21:56:15.690446Z","shell.execute_reply":"2023-09-15T21:56:21.412452Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'ERA-V1'...\nremote: Enumerating objects: 934, done.\u001b[K\nremote: Counting objects: 100% (126/126), done.\u001b[K\nremote: Compressing objects: 100% (94/94), done.\u001b[K\nremote: Total 934 (delta 68), reused 82 (delta 32), pack-reused 808\u001b[K\nReceiving objects: 100% (934/934), 54.93 MiB | 28.75 MiB/s, done.\nResolving deltas: 100% (467/467), done.\nAlready up to date.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch.nn.functional as F\nfrom collections import Counter\nfrom os.path import exists\nimport torch.optim as optim\nimport torch.nn as nn\nimport numpy as np\nimport random\nimport torch\nimport math\nimport re\nfrom transformer import BERT","metadata":{"execution":{"iopub.status.busy":"2023-09-15T22:00:06.506224Z","iopub.execute_input":"2023-09-15T22:00:06.506634Z","iopub.status.idle":"2023-09-15T22:00:06.517199Z","shell.execute_reply.started":"2023-09-15T22:00:06.506599Z","shell.execute_reply":"2023-09-15T22:00:06.516068Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2023-09-15T21:58:19.275208Z","iopub.execute_input":"2023-09-15T21:58:19.275609Z","iopub.status.idle":"2023-09-15T21:58:20.320546Z","shell.execute_reply.started":"2023-09-15T21:58:19.275578Z","shell.execute_reply":"2023-09-15T21:58:20.319243Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34mERA-V1\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":"# =============================================================================\n# Dataset\n# =============================================================================\nclass SentencesDataset(Dataset):\n    #Init dataset\n    def __init__(self, sentences, vocab, seq_len):\n        dataset = self\n        \n        dataset.sentences = sentences\n        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n        dataset.seq_len = seq_len\n        \n        #special tags\n        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n    \n    \n    #fetch data\n    def __getitem__(self, index, p_random_mask=0.15):\n        dataset = self\n        \n        #while we don't have enough word to fill the sentence for a batch\n        s = []\n        while len(s) < dataset.seq_len:\n            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n            index += 1\n        \n        #ensure that the sequence is of length seq_len\n        s = s[:dataset.seq_len]\n        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n        \n        #apply random mask\n        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n        \n        return {'input': torch.Tensor([w[0] for w in s]).long(),\n                'target': torch.Tensor([w[1] for w in s]).long()}\n\n    #return length\n    def __len__(self):\n        return len(self.sentences)\n\n    #get words id\n    def get_sentence_idx(self, index):\n        dataset = self\n        s = dataset.sentences[index]\n        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n        return s\n\n# =============================================================================\n# Methods / Class\n# =============================================================================\ndef get_batch(loader, loader_iter):\n    try:\n        batch = next(loader_iter)\n    except StopIteration:\n        loader_iter = iter(loader)\n        batch = next(loader_iter)\n    return batch, loader_iter\n\n# =============================================================================\n# #Init\n# =============================================================================\nprint('initializing..')\nbatch_size = 1024\nseq_len = 20\nembed_size = 128\ninner_ff_size = embed_size * 4\nn_heads = 8\nn_code = 8\nn_vocab = 40000\ndropout = 0.1\n# n_workers = 12\n\n#optimizer\noptim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n\n# =============================================================================\n# Input\n# =============================================================================\n#1) load text\nprint('loading text...')\npth = 'ERA-V1/ERA-V1-Session-17/BERT/training.txt'\nsentences = open(pth).read().lower().split('\\n')\n\n#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\nprint('tokenizing sentences...')\nspecial_chars = ',?;.:/*!+-()[]{}\"\\'&'\nsentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\nsentences = [[w for w in s if len(w)] for s in sentences]\n\n#3) create vocab if not already created\nprint('creating/loading vocab...')\npth = 'ERA-V1/ERA-V1-Session-17/BERT/vocab.txt'\nif not exists(pth):\n    words = [w for s in sentences for w in s]\n    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n    vocab = [w[0] for w in vocab]\n    open(pth, 'w+').write('\\n'.join(vocab))\nelse:\n    vocab = open(pth).read().split('\\n')\n\n#4) create dataset\nprint('creating dataset...')\ndataset = SentencesDataset(sentences, vocab, seq_len)\n# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\nkwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\ndata_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n\n\n# =============================================================================\n# Model\n# =============================================================================\n#init model\nprint('initializing model...')\nmodel = BERT(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\nmodel = model.cuda()\n\n# =============================================================================\n# Optimizer\n# =============================================================================\nprint('initializing optimizer and loss...')\noptimizer = optim.Adam(model.parameters(), **optim_kwargs)\nloss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n\n# =============================================================================\n# Train\n# =============================================================================\nprint('training...')\nprint_each = 10\nmodel.train()\nbatch_iter = iter(data_loader)\nn_iteration = 10000\nfor it in range(n_iteration):\n    \n    #get batch\n    batch, batch_iter = get_batch(data_loader, batch_iter)\n    \n    #infer\n    masked_input = batch['input']\n    masked_target = batch['target']\n    \n    masked_input = masked_input.cuda(non_blocking=True)\n    masked_target = masked_target.cuda(non_blocking=True)\n    output = model(masked_input)\n    \n    #compute the cross entropy loss \n    output_v = output.view(-1,output.shape[-1])\n    target_v = masked_target.view(-1,1).squeeze()\n    loss = loss_model(output_v, target_v)\n    \n    #compute gradients\n    loss.backward()\n    \n    #apply gradients\n    optimizer.step()\n    \n    #print step\n    if it % print_each == 0:\n        print('it:', it, \n              ' | loss', np.round(loss.item(),2),\n              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n    \n    #reset gradients\n    optimizer.zero_grad()\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T22:09:24.188902Z","iopub.execute_input":"2023-09-15T22:09:24.189306Z","iopub.status.idle":"2023-09-15T22:19:41.884391Z","shell.execute_reply.started":"2023-09-15T22:09:24.189275Z","shell.execute_reply":"2023-09-15T22:19:41.881845Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"initializing..\nloading text...\ntokenizing sentences...\ncreating/loading vocab...\ncreating dataset...\ninitializing model...\ninitializing optimizer and loss...\ntraining...\nit: 0  | loss 10.26  | Δw: 1.147\nit: 10  | loss 9.58  | Δw: 0.543\nit: 20  | loss 9.33  | Δw: 0.336\nit: 30  | loss 9.16  | Δw: 0.28\nit: 40  | loss 9.06  | Δw: 0.236\nit: 50  | loss 8.89  | Δw: 0.213\nit: 60  | loss 8.74  | Δw: 0.2\nit: 70  | loss 8.59  | Δw: 0.186\nit: 80  | loss 8.46  | Δw: 0.176\nit: 90  | loss 8.24  | Δw: 0.168\nit: 100  | loss 8.1  | Δw: 0.162\nit: 110  | loss 7.95  | Δw: 0.157\nit: 120  | loss 7.87  | Δw: 0.145\nit: 130  | loss 7.69  | Δw: 0.139\nit: 140  | loss 7.52  | Δw: 0.141\nit: 150  | loss 7.49  | Δw: 0.134\nit: 160  | loss 7.38  | Δw: 0.129\nit: 170  | loss 7.28  | Δw: 0.127\nit: 180  | loss 7.17  | Δw: 0.133\nit: 190  | loss 7.08  | Δw: 0.122\nit: 200  | loss 6.93  | Δw: 0.122\nit: 210  | loss 6.86  | Δw: 0.121\nit: 220  | loss 6.76  | Δw: 0.124\nit: 230  | loss 6.74  | Δw: 0.12\nit: 240  | loss 6.74  | Δw: 0.125\nit: 250  | loss 6.59  | Δw: 0.123\nit: 260  | loss 6.69  | Δw: 0.117\nit: 270  | loss 6.49  | Δw: 0.123\nit: 280  | loss 6.55  | Δw: 0.129\nit: 290  | loss 6.46  | Δw: 0.13\nit: 300  | loss 6.48  | Δw: 0.132\nit: 310  | loss 6.49  | Δw: 0.141\nit: 320  | loss 6.34  | Δw: 0.147\nit: 330  | loss 6.49  | Δw: 0.154\nit: 340  | loss 6.39  | Δw: 0.157\nit: 350  | loss 6.37  | Δw: 0.17\nit: 360  | loss 6.39  | Δw: 0.185\nit: 370  | loss 6.39  | Δw: 0.201\nit: 380  | loss 6.39  | Δw: 0.204\nit: 390  | loss 6.4  | Δw: 0.215\nit: 400  | loss 6.42  | Δw: 0.235\nit: 410  | loss 6.38  | Δw: 0.283\nit: 420  | loss 6.32  | Δw: 0.283\nit: 430  | loss 6.33  | Δw: 0.313\nit: 440  | loss 6.32  | Δw: 0.348\nit: 450  | loss 6.31  | Δw: 0.374\nit: 460  | loss 6.37  | Δw: 0.346\nit: 470  | loss 6.3  | Δw: 0.377\nit: 480  | loss 6.36  | Δw: 0.415\nit: 490  | loss 6.38  | Δw: 0.476\nit: 500  | loss 6.34  | Δw: 0.496\nit: 510  | loss 6.35  | Δw: 0.5\nit: 520  | loss 6.28  | Δw: 0.502\nit: 530  | loss 6.36  | Δw: 0.565\nit: 540  | loss 6.34  | Δw: 0.555\nit: 550  | loss 6.33  | Δw: 0.591\nit: 560  | loss 6.24  | Δw: 0.603\nit: 570  | loss 6.23  | Δw: 0.63\nit: 580  | loss 6.33  | Δw: 0.691\nit: 590  | loss 6.3  | Δw: 0.69\nit: 600  | loss 6.28  | Δw: 0.812\nit: 610  | loss 6.18  | Δw: 0.859\nit: 620  | loss 6.19  | Δw: 0.802\nit: 630  | loss 6.31  | Δw: 0.763\nit: 640  | loss 6.21  | Δw: 0.819\nit: 650  | loss 6.34  | Δw: 0.839\nit: 660  | loss 6.21  | Δw: 0.865\nit: 670  | loss 6.28  | Δw: 0.863\nit: 680  | loss 6.22  | Δw: 0.974\nit: 690  | loss 6.27  | Δw: 0.922\nit: 700  | loss 6.29  | Δw: 0.931\nit: 710  | loss 6.25  | Δw: 1.001\nit: 720  | loss 6.23  | Δw: 1.011\nit: 730  | loss 6.23  | Δw: 1.014\nit: 740  | loss 6.26  | Δw: 1.079\nit: 750  | loss 6.27  | Δw: 0.99\nit: 760  | loss 6.17  | Δw: 1.128\nit: 770  | loss 6.25  | Δw: 1.078\nit: 780  | loss 6.13  | Δw: 1.085\nit: 790  | loss 6.15  | Δw: 1.176\nit: 800  | loss 6.22  | Δw: 1.181\nit: 810  | loss 6.11  | Δw: 1.206\nit: 820  | loss 6.14  | Δw: 1.174\nit: 830  | loss 6.16  | Δw: 1.223\nit: 840  | loss 6.22  | Δw: 1.445\nit: 850  | loss 6.25  | Δw: 1.261\nit: 860  | loss 6.22  | Δw: 1.341\nit: 870  | loss 6.17  | Δw: 1.295\nit: 880  | loss 6.24  | Δw: 1.275\nit: 890  | loss 6.11  | Δw: 1.313\nit: 900  | loss 6.13  | Δw: 1.411\nit: 910  | loss 6.27  | Δw: 1.546\nit: 920  | loss 6.18  | Δw: 1.411\nit: 930  | loss 6.13  | Δw: 1.372\nit: 940  | loss 6.22  | Δw: 1.338\nit: 950  | loss 6.17  | Δw: 1.425\nit: 960  | loss 6.04  | Δw: 1.539\nit: 970  | loss 6.11  | Δw: 1.51\nit: 980  | loss 6.13  | Δw: 1.474\nit: 990  | loss 6.19  | Δw: 1.454\nit: 1000  | loss 6.12  | Δw: 1.567\nit: 1010  | loss 6.09  | Δw: 1.524\nit: 1020  | loss 6.2  | Δw: 1.559\nit: 1030  | loss 6.08  | Δw: 1.734\nit: 1040  | loss 6.09  | Δw: 1.663\nit: 1050  | loss 6.06  | Δw: 1.715\nit: 1060  | loss 6.06  | Δw: 1.559\nit: 1070  | loss 6.12  | Δw: 1.621\nit: 1080  | loss 6.08  | Δw: 1.508\nit: 1090  | loss 6.09  | Δw: 1.671\nit: 1100  | loss 6.03  | Δw: 1.708\nit: 1110  | loss 6.06  | Δw: 1.75\nit: 1120  | loss 5.94  | Δw: 1.888\nit: 1130  | loss 6.02  | Δw: 1.675\nit: 1140  | loss 6.1  | Δw: 1.784\nit: 1150  | loss 6.07  | Δw: 1.945\nit: 1160  | loss 6.11  | Δw: 1.899\nit: 1170  | loss 6.01  | Δw: 1.771\nit: 1180  | loss 6.04  | Δw: 2.109\nit: 1190  | loss 6.04  | Δw: 2.106\nit: 1200  | loss 6.08  | Δw: 2.028\nit: 1210  | loss 6.07  | Δw: 1.994\nit: 1220  | loss 6.05  | Δw: 1.956\nit: 1230  | loss 6.03  | Δw: 1.948\nit: 1240  | loss 6.06  | Δw: 2.213\nit: 1250  | loss 6.03  | Δw: 2.169\nit: 1260  | loss 5.99  | Δw: 2.173\nit: 1270  | loss 5.98  | Δw: 2.109\nit: 1280  | loss 6.03  | Δw: 2.138\nit: 1290  | loss 6.01  | Δw: 2.081\nit: 1300  | loss 6.02  | Δw: 2.081\nit: 1310  | loss 5.92  | Δw: 2.142\nit: 1320  | loss 5.96  | Δw: 2.224\nit: 1330  | loss 5.95  | Δw: 2.445\nit: 1340  | loss 5.96  | Δw: 2.45\nit: 1350  | loss 5.93  | Δw: 2.388\nit: 1360  | loss 5.82  | Δw: 2.615\nit: 1370  | loss 5.88  | Δw: 2.489\nit: 1380  | loss 5.88  | Δw: 2.479\nit: 1390  | loss 5.93  | Δw: 2.606\nit: 1400  | loss 5.87  | Δw: 2.532\nit: 1410  | loss 5.88  | Δw: 2.713\nit: 1420  | loss 5.86  | Δw: 2.505\nit: 1430  | loss 5.9  | Δw: 2.792\nit: 1440  | loss 5.86  | Δw: 2.662\nit: 1450  | loss 5.87  | Δw: 2.878\nit: 1460  | loss 5.85  | Δw: 2.739\nit: 1470  | loss 5.74  | Δw: 2.748\nit: 1480  | loss 5.89  | Δw: 2.793\nit: 1490  | loss 5.84  | Δw: 2.834\nit: 1500  | loss 5.88  | Δw: 2.804\nit: 1510  | loss 5.77  | Δw: 2.787\nit: 1520  | loss 5.85  | Δw: 2.763\nit: 1530  | loss 5.74  | Δw: 2.976\nit: 1540  | loss 5.85  | Δw: 3.178\nit: 1550  | loss 5.83  | Δw: 3.063\nit: 1560  | loss 5.77  | Δw: 3.015\nit: 1570  | loss 5.77  | Δw: 2.983\nit: 1580  | loss 5.79  | Δw: 2.787\nit: 1590  | loss 5.82  | Δw: 3.208\nit: 1600  | loss 5.84  | Δw: 3.038\nit: 1610  | loss 5.91  | Δw: 3.042\nit: 1620  | loss 5.83  | Δw: 3.113\nit: 1630  | loss 5.86  | Δw: 3.403\nit: 1640  | loss 5.72  | Δw: 3.205\nit: 1650  | loss 5.78  | Δw: 3.44\nit: 1660  | loss 5.68  | Δw: 3.203\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 155\u001b[0m\n\u001b[1;32m    152\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_model(output_v, target_v)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m#compute gradients\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m#apply gradients\u001b[39;00m\n\u001b[1;32m    158\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# =============================================================================\n# Results analysis\n# =============================================================================\nprint('saving embeddings...')\nN = 3000\nnp.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\ns = [dataset.rvocab[i] for i in range(N)]\nopen('ERA-V1/ERA-V1-Session-17/BERT/names.tsv', 'w+').write('\\n'.join(s) )\n\n\nprint('end')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T22:19:54.742557Z","iopub.execute_input":"2023-09-15T22:19:54.742997Z","iopub.status.idle":"2023-09-15T22:19:54.939571Z","shell.execute_reply.started":"2023-09-15T22:19:54.742944Z","shell.execute_reply":"2023-09-15T22:19:54.938462Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"saving embeddings...\nend\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}